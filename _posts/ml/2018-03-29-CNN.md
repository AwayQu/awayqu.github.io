---
layout: post
title:  "CNN"
date:   2018-03-29 00:35:23 +0800
categories: ML
---

# Convolutional Neural Networks (学习笔记)


# Convolutional Layer (卷积层)
## convolution nenuron model 卷积神经元模型

![image.png]({{ site.img_url }}imgs/ml/neuron_model.jpeg)

## 卷积公式

![image.png]({{ site.img_url }}imgs/ml/convolution_function.png)

** 前一个轴突的输出 `X` 与 权重矩阵 `θ` 的 `点乘积` 加上 偏置  然后在导入activation function 激励函数**

## filter

 [size * size * 3] 的 权重矩阵 `θ` 


## 对于图片卷积

对于 [27 X 27 X 3] 的 输入

可以应用  [size * size * 3] 的过滤器 （权重矩阵 `θ`），如果应用的（filter）过滤器 有12个，则得到 容积为 [27 X 27 X 12] 的输出。

12 是 activation volume(活化容积) 的深度。

也就是说 

> 3D的图像矩阵（输入）通过 过滤器 （权重矩阵） 卷积并且应用激励函数 获取到 N 维（深度）的 活化容积（输出）

# RELU layer

> ReLU，线性整流函数 (Rectified Linear Unit)

会应用按元素的激励函数（elementwise activation function），如such as the max(0,x) 

# pooling layer (池化层)

> 执行降缩样本操作，缩小 空间维度(width, height)的样本量 

# FC (i.e. fully-connected) layer （全连接层）


# 算法

argmax()

softmax()

cross entropy

softmax_cross_entropy

tf.one_hot()
<!-- 
 because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.
-->

# connectionist temporal classification （CTC)

# Long Short-Term Memory (LSTM)

# Convolutional Recursive  Neural Networks (CRNN)

# 名词
**减小特征range 减少梯度向下次数**
feature scaling  特性缩放
mean normalization 均值归一化
gradient desent 梯度向下
learning rate 学习效率
invertibility 可以
convergence 汇聚
convex 凸面体
overfitting 过度拟合
underfitting 欠拟合
high bias 高偏差
polynomial 多项式
regularization 正则化
high variance 高方差
generalize 泛化
magnitude 量级
polynomial regression 多项式回归
penalize 处罚
prone 倾向
normal equation 正规方程
sigmoid s弯型

downsampling operation 降缩样本操作
spatial dimensions 空间维度
activation volume 活化容积
weight 权重
volume 容积
activation function 激活函数
axon 轴突
synapse 突触
bias 偏置


connectionist-temporal-classification